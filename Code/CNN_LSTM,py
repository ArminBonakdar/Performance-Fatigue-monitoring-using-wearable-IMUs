# ---------------------- Imports ----------------------
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import LeaveOneGroupOut
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, confusion_matrix, roc_auc_score
)
from tensorflow.keras import models, layers
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout
from tensorflow.keras.utils import to_categorical

# ---------------------- Load & Concatenate Excel Sheets ----------------------
file_path = 'drive/MyDrive/NCB-Armin/Fatigue analysis/Main_project/IMU_MainProject/Data/JointAngle_C_data.xlsx'

sheets = [pd.read_excel(file_path, sheet_name=i).iloc[1:] for i in range(3)]
data_df = pd.concat(sheets, ignore_index=True)
print("Combined dataset shape:", data_df.shape)

# ---------------------- Data Preparation ----------------------
# Assume 'features', 'fatigue', and 'subject' columns already extracted properly from `data_df`
features = data_df.iloc[:, 2:8].values  # 6 IMU joint angle features
fatigue = data_df['Label'].values
subject = data_df['Subject'].values

# Scale features
scaler = StandardScaler()
X = scaler.fit_transform(np.array(features).reshape(-1, 6))
y = np.array(fatigue).flatten()
z = np.array(subject)

# ---------------------- Windowing ----------------------
sequence_length = 100
X_reshaped, y_reshaped, z_reshaped = [], [], []

for i in range(len(X) - sequence_length + 1):
    X_seq = X[i:i + sequence_length]
    y_seq = y[i:i + sequence_length]
    z_seq = z[i:i + sequence_length]

    if all(label == y_seq[0] for label in y_seq):
        X_reshaped.append(X_seq)
        y_reshaped.append(y_seq[0])
        z_reshaped.append(z_seq[0])

X_reshaped = np.array(X_reshaped)
y_reshaped = np.array(y_reshaped)
z_reshaped = np.array(z_reshaped)

print("Windowed shape:", X_reshaped.shape)

# ---------------------- One-Hot Encoding ----------------------
num_classes = len(np.unique(y_reshaped))
y_one_hot = to_categorical(y_reshaped, num_classes=num_classes)

# ---------------------- CNN-LSTM Model Definition ----------------------
def build_cnn_lstm_model(input_shape, num_classes):
    model = models.Sequential()
    model.add(Conv1D(64, kernel_size=3, activation='tanh', padding='same', input_shape=input_shape))
    model.add(MaxPooling1D(pool_size=2, padding='same'))
    model.add(Conv1D(32, kernel_size=3, activation='tanh', padding='same'))
    model.add(MaxPooling1D(pool_size=2, padding='same'))
    model.add(LSTM(32, activation='tanh', return_sequences=False))
    model.add(Dense(10, activation='tanh'))
    model.add(Dense(32, activation='tanh'))
    model.add(Dropout(0.2))
    model.add(Dense(16, activation='tanh'))
    model.add(Dropout(0.2))
    model.add(Dense(8, activation='tanh'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# ---------------------- LOSO Cross-Validation ----------------------
cv_accuracy, cv_precision, cv_recall, cv_f1, cv_auc, train_accuracy_list, cv_cm = [], [], [], [], [], [], []
logo = LeaveOneGroupOut()

for i, (train_idx, test_idx) in enumerate(logo.split(X_reshaped, y_reshaped, groups=z_reshaped)):
    print(f"\n--- Fold {i+1}/{len(np.unique(z_reshaped))} ---")
    model = build_cnn_lstm_model(input_shape=(X_reshaped.shape[1], X_reshaped.shape[2]), num_classes=num_classes)

    X_train, X_test = X_reshaped[train_idx], X_reshaped[test_idx]
    y_train, y_test = y_one_hot[train_idx], y_one_hot[test_idx]

    model.fit(X_train, y_train, epochs=30, batch_size=16, verbose=0)

    y_pred_prob = model.predict(X_test)
    y_pred = np.argmax(y_pred_prob, axis=1)
    y_true = np.argmax(y_test, axis=1)

    # Evaluation
    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)
    rec = recall_score(y_true, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
    auc = roc_auc_score(y_test, y_pred_prob, multi_class='ovo', average='macro')

    train_pred = model.predict(X_train)
    train_accuracy = accuracy_score(np.argmax(y_train, axis=1), np.argmax(train_pred, axis=1))

    # Store metrics
    cv_accuracy.append(acc)
    cv_precision.append(prec)
    cv_recall.append(rec)
    cv_f1.append(f1)
    cv_auc.append(auc)
    train_accuracy_list.append(train_accuracy)

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    cv_cm.append(cm)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - Fold {i+1}')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

# ---------------------- Final Report ----------------------
print("\n=== Average Cross-Validation Metrics ===")
print(f"Mean Accuracy     : {np.mean(cv_accuracy):.3f}")
print(f"Mean Precision    : {np.mean(cv_precision):.3f}")
print(f"Mean Recall       : {np.mean(cv_recall):.3f}")
print(f"Mean F1 Score     : {np.mean(cv_f1):.3f}")
print(f"Mean Train Accuracy: {np.mean(train_accuracy_list):.3f}")
print(f"Mean ROC AUC      : {np.mean(cv_auc):.3f}")
